---
title: "Wavelet Analysis of Variance"
author: "Erik Dean"
date: "1/21/2022"
output: html_document
---

```{r setup, include=FALSE}

# don't output warnings, code, messages in markdown
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = F) 

# import packages we need (and maybe some we don't). 
# run install.packages("") on each of these if you haven't before

library(dplyr)
library(tidyr)
library(purrr)
library(broom)
library(lubridate)
library(gtsummary)
library(tidyverse)
library(plotly)
library(reshape)
library(viridis)
library(tibble)
library(forecast)
library(quantmod)
library(fpp3)
library(zoo)
library(vars)
library(WaveletComp)
library(wavethresh)
library(waveslim)
library(pander)

# set color theme for plots

scale_colour_brewer_d <- function(...) {
  scale_colour_brewer(palette = "Dark2", ...)
}

scale_fill_brewer_d <- function(...) {
  scale_fill_brewer(palette = "Dark2", ...)
}

options(
  ggplot2.discrete.colour = scale_colour_brewer_d,
  ggplot2.discrete.fill = scale_fill_brewer_d
)

```

## Description

This RMD covers analysis of variance using discrete wavelet tranforms.  I'll pull mostly from Gencay et al (2001) and the waveslim package.  The ultimate goal here is to break categorize our inflation data according to fine vs. coarse variance--that is, items whose 'energy' occurs mainly at high frequencies (say, a few months) versus lower frequences (say a couple years). Hopefully, this will give us something like the categories that Gardiner Means produced as administered prices versus market prices.

The sections that follow, I will...

* Import the BEA data (see Process PCE Data.Rmd)
* Provide code for querying the data
* Analyze the energy composition of the price data

```{r import and format tables, echo = F}

#clear the stuff

rm(list = ls())

# import the tables

expenditures <- read.csv("243U.csv", header = F, na.strings = "---")
prices <- read.csv("244U.csv", header = F, na.strings = "---")
quantities <- read.csv("245U.csv", header = F, na.strings = "---")

# format expenditures tables

  # remove title rows (1:4) and footnotes (410:414)
  exp <- expenditures[5:413, ]
  
  # move aggregates to separate table
  aggExp <- exp[c(3, 369:409), ]
  exp <- exp[-c(3, 369:409), ]
  
  # move non-profit institutions serving households (NPISHs) to separate table
  npishExp <- exp[341:367, ]
  exp <- exp[-(341:367), ]
  
  # combine month and year data into 2nd row
  exp[2,3:ncol(exp)] <- as.character(my(paste0(exp[2,3:ncol(exp)], " ", exp[1,3:ncol(exp)])))
  exp <- exp[-1,]
  exp[1, 2] <- "Date"

  # add the date rows to the other tables
  aggExp <- rbind(exp[1,], aggExp)
  npishExp <- rbind(exp[1,], npishExp)

# format prices tables (same as above)

  pri <- prices[5:413, ]
  aggPri <- pri[c(3, 369:409), ]
  pri <- pri[-c(3, 369:409), ]
  npishPri <- pri[341:367, ]
  pri <- pri[-(341:367), ]
  pri[2,3:ncol(pri)] <- as.character(my(paste0(pri[2,3:ncol(pri)], " ", pri[1,3:ncol(pri)])))
  pri <- pri[-1,]
  pri[1, 2] <- "Date"
  aggPri <- rbind(pri[1,], aggPri)
  npishPri <- rbind(pri[1,], npishPri)

# format qua tables (same as above)

  qua <- quantities[5:413, ]
  aggQua <- qua[c(3, 369:409), ]
  qua <- qua[-c(3, 369:409), ]
  npishQua <- qua[341:367, ]
  qua <- qua[-(341:367), ]
  qua[2,3:ncol(qua)] <- as.character(my(paste0(qua[2,3:ncol(qua)], " ", qua[1,3:ncol(qua)])))
  qua <- qua[-1,]
  qua[1, 2] <- "Date"
  aggQua <- rbind(qua[1,], aggQua)
  npishQua <- rbind(qua[1,], npishQua)
  
# remove the original tables
  
rm(expenditures)
rm(prices)
rm(quantities)
  
```

## Query the Data

The following function allows querying the main tables (exp, qua, and pri) based on item levels, where goods and services are level 1; durable goods, nondurable goods, and HH cons exp on services are level 2; and so on.  Often, we'll want the lowest (i.e. most granular level), which can be retrieved with the lowestLevel = T parameter.  This is the same as in Process PCE Data.Rmd

```{r make ts function, echo = F}

makeTimeSeries <- function(df, startDate = "1900-01-01", endDate = "2500-01-01", onlyLowestLevel = T, removeNAs = T){

  #df <- pri
  #startDate <- "1980-01-01"
  #endDate = "1990-01-01"
  
  # make data params actual dates
  startDate = ymd(startDate)
  endDate = ymd(endDate)
  
  #########################################
  # adjust start and end dates
  
  x <- ymd(df[1, 3:ncol(df)]) # get the dates from the tables

  # if specified start/end date is/are out of bounds, then set it/them to the first/last date
  if (startDate < x[1]) { startDate <- x[1] }
  if (endDate > x[length(x)]) { endDate <- x[length(x)] }
  
  cutLateYears <- ncol(df) - sum(x > endDate)
  
  if(cutLateYears <= ncol(df)){ 
    df <- df[, 1:cutLateYears] 
  }
  
  cutEarlyYears <- sum(x < startDate)
  
  if(cutEarlyYears > 1){ 
    df <- df[,-(3:(cutEarlyYears+2))] 
  }
   
  #########################################
  # remove rows with NAs
  # this will ensure that levels that have sub-levels but not full data for them get marked as the lowest level
  # but those lower levels will be dropped. usually, NAs are due to new products, 
  # so what gets cut out here will depend on the time frame, which is why I've run the start/end dates cut prior to this
  
  if (removeNAs){ df <- na.omit(df) }
  
  ############################################
  ### create a new level system for categories
  
  # find number of preceding spaces (the function tells us position of first non-space, so subtract one from that)
  # (and there are 4 spaces for each additional levels, so divide by 4)
  numSpac <- (regexpr("[A-Z0-9]", df[, 2]) - 1) / 4
  
  n <- 2 # start off on the 2nd row, the first is a header row 
  # current level (higher number means lower sublevel), there won't be a 9th level, it's there so the 2nd loop doesn't throw NAs
  cl <- c(0, 0, 0, 0, 0, 0, 0, 0, 0) 
  upLvl <- 0 # goes up by one if we move back to a higher category level
  lp <- 1 # current position in level
  
  level <- vector("list", nrow(df)) # this will be the actual level code
  level[[1]] <- "Level"
  lowestLevel <- vector("list", nrow(df)) # this will indicate if the current level has no sub-levels
  lowestLevel[[1]] <- "lowestLevel"

  while (n <= nrow(df)){
    if (numSpac[n] > numSpac[n-1]){ # move to sublevel, set previous row's lowestLevel to 0, and zero out upLvl
      lp <- lp + 1  
    }
    else if (numSpac[n] < numSpac[n-1]){ # move to higher level
      lp <- lp - (numSpac[n-1] - numSpac[n]) # determine level position based on how far we dropped back
      cl[lp+1:8] <- 0 # zero out levels to right of this higher level

    }
    cl[lp] <- cl[lp] + 1
    
    level[n] <- paste(cl[1], cl[2], cl[3], cl[4], cl[5], cl[6], cl[7], cl[8], cl[9], sep = ".")
    n <- n + 1
  }
  
  # add the level column to the tables
  df <- add_column(df, "Level" = level, .after = 1)
  
  n <- 2
  while (n < nrow(df)){  
    # if the first zero in the current row appears earlier than in the next row, then the next item is a sublevel and this one isn't the lowest
    if (str_locate(df$Level[n], "0")[1] < str_locate(df$Level[n+1], "0")[1]){
      lowestLevel[n] <- 0
    } else { lowestLevel[n] <- 1 }
    n <- n + 1
  }
  # the loop won't pick up the last the row so do that manually
  # it should always be 'other household services', which is a lowest level
  lowestLevel[nrow(df)] <- 1
  
  # add the lowestLevel column to the tables
  df <- add_column(df, "lowestLevel" = lowestLevel, .after = 2)
  
  # move first row to column names
  names(df) <- as.character(unlist(df[1,]))
  colnames(df)[2] <- "Category"
  
  # create bridge between level number and product category
  if (onlyLowestLevel) { df  <- df[df$lowestLevel == 1, ] }
  bridgeLvlCat <- df[,c(1, 2, 4)]
  df <- df[-1,]
  bridgeLvlCat <- bridgeLvlCat[-1,]
  
  # make level numbers row names
  row.names(df) <- df[,2]

  # transpose and pare down the base tables to only include the lowest levels
  # if a category has sub-levels those are included but not the higher level (that the lower ones aggregate to)

  df <- as.data.frame(t(df[,-(1:4)]))
  df[,] <- as.numeric(unlist(df[,]))

  # make the time series
  df.ts <- ts(df, start = c(substr(startDate, 1, 4), substr(startDate, 6, 7)), end = c(substr(endDate, 1, 4), substr(endDate, 6, 7)), freq = 12)

  # remove stuff
  #rm(level, cl, lp, n, numSpac, upLvl, lowestLevel, cutEarlyYears, cutLateYears, startDate, endDate, x, onlyLowestLevel, removeNAs)

  output <- list(item = df, level = as.character(bridgeLvlCat[, 2]), name = trimws(bridgeLvlCat[, 3]), 
                 line = bridgeLvlCat[, 1], startDate = startDate, endDate = endDate)

  return(output)
}

```


```{r plot a series, eval = F}

endDate <- as.Date("2021-11-01")
startDate <- as.Date("1970-01-01")

# get the table from the function
quaSeries <- makeTimeSeries(qua, startDate = startDate, endDate = endDate)

# selection an item by number
i <- 130

# make it a time series (for some reason I could never get it to come out a ts from the function)
series <- ts(quaSeries$item[i], start = as.numeric(substr(quaSeries$startDate, 1, 4)), end = as.numeric(substr(quaSeries$endDate, 1, 4)), freq = 12)

# plot the series
#plot(series, main = quaSeries$name[i])

```

## Energy Analysis

Energy is defined as the sum of squared values of a vector.  Energy is proportional to variance, and the discrete wavelet transform is energy (variance) preserving.  Hence the sum of squared values of a time series (x) equals the sum of the sum of squared wavelet scale coefficients (d) across all scales (1-J), including the smooth (s).  Gencay et al. (2001, 125) write this as

$||x||^{2} = \sum_{j = 1}^{J}||d_j||^2 + ||s_J||^2$

Where $||.||^2$ is just the sum of the squared values in the vector.  Hence the sum of the squared values of the original series equals the sum of the squared coefficients of all of the wavelet scales, including the smooth.

Using data from IBM's stock returns in the 1960s, Gencay et al. (2001, 127-8) plot the wavelet energies "normalized by $N^{-1}$," which is to say they take the sum of squared coefficients then divide by length (i.e. number of coefficients) for each scale.  Dividing by number of coefficients is necessary because, by definition, each higher (coarser) scale will have half as many observations as the (finer) scale below it, such that finer scales will typically have higher sums of squared coefficients simply for having much larger numbers of coefficients.  

In terms of choosing the wavelet, the authors also note that as "the length of the wavelet filter increases, the approximation to an ideal band-pass filter improves and therefore the wavelet filter will better capture the variability in the frequency intervals associated with the DWT wavelet coefficients."  Hence, below I will use the LA(8) wavelet, not the Haar (which has length 2).

So the idea here to to see what frequencies have the most energy--which is similar to asking whether the time series has a lot of long-period versus short-period variance. To demonstrate, the following will decompose the energy of the DWT for a single item.  Note, our data has around 745 months.  We may move on to MODWTs later, which don't care about this, but standard DWTs need dyadic lengths ($2^2 = 4$, ($2^3 = 8$, 16, 32, 64, 128, 256, &c.). _However_, partial DWTs (see Gencay et al. 2001, 124) should work just as well and since we really don't care about frequencies beyond a business cycle (which we'll call 128 months at most), then we really just need a sample size divisible by 128.  This means 640 should work for us, although I should note that this will remove some items that don't show up in the series until later.

```{r bar graph of energies}

# get start and end dates to make a series length of 640
endDate <- as.Date("2021-11-01")
startDate <- as.Date(endDate) %m-% months(640)

# get the table from the function
priSeries <- makeTimeSeries(pri, startDate = startDate, endDate = endDate)

# start date moves up a month for the difference (below)
startDate <- as.Date(startDate) %m+% months(1)

# selection an item by number
i <- 50

itemTable <- data.frame()

  # take the first differences (I don't see a particular reason to take logs, although Gencay et al. usually do)
  diff <- diff(priSeries$item[[i]])
  
  # make it a time series (for some reason I could never get it to come out a ts from the function)
  
  startYM <- as.character(startDate) # I'm sure there's a better way of doing this, but it works
  endYM <- as.character(endDate)
  
  series <- ts(diff, start = c(substr(startYM, 1, 4), substr(startYM, 6, 7)), end = c(substr(endYM, 1, 4), substr(endYM, 6, 7)), freq = 12)
  
  ## (partial) Discrete Wavelet Transform with the LA(8) wavelet
  priDWT <- dwt(series, "la8", 7)
  names(priDWT) <- c("d1", "d2", "d3", "d4", "d5", "d6", "d7", "s7")
  
  # now compare the energies for each scale (but not the smooth)
  
  energies <- vector()
  
  for(x in 1:7){
    energies[x] <- sum(priDWT[[x]]^2)/length(priDWT[[x]])
  }
  scaleNames <- c(1:7)

  # plot the energies
  barplot(energies ~ scaleNames, xlab = "Scale", main = priSeries$name[i])

```

You can see that eggs have a fairly high degree of energy (the average scale energy, excluding the smooth, across all scales and items came out to about 2, partly raised by extremely high energies for securities commissions). But you can also see that most of that energy is in scales 4 and 5, which (and I'll need to check this to be sure I'm reading it right), corresponds to 16-32 and 32-64 month periods--i.e. we're looking mostly at price changes between about 1-5 years.

## Scale Energies for All Items

The following will produce a table of all (least aggregated) items for this time period, including whether they're durable goods, nondurable goods, or services, and their energies by scale (not including the smooth). The table is ordered by d1 energy (that is, the energy of the finest scale, which I believe represents prices changes over 2-4 months).

```{r energies for all items}

# get start and end dates to make a series length of 640
endDate <- as.Date("2021-11-01")
startDate <- as.Date(endDate) %m-% months(640)

# get the table from the function
priSeries <- makeTimeSeries(pri, startDate = startDate, endDate = endDate)

# start date moves up a month for the difference (below)
startDate <- as.Date(startDate) %m+% months(1)

# selection an item by number
i <- 1

itemTable <- data.frame()

for(i in 1:(length(priSeries$level))){

  # take the first differences (I don't see a particular reason to take logs, although Gencay et al. usually do)
  diff <- diff(priSeries$item[[i]])
  
  # make it a time series (for some reason I could never get it to come out a ts from the function)
  
  startYM <- as.character(startDate) # I'm sure there's a better way of doing this, but it works
  endYM <- as.character(endDate)
  
  series <- ts(diff, start = c(substr(startYM, 1, 4), substr(startYM, 6, 7)), end = c(substr(endYM, 1, 4), substr(endYM, 6, 7)), freq = 12)
  
  ## (partial) Discrete Wavelet Transform with the LA(8) wavelet
  priDWT <- dwt(series, "la8", 7)
  names(priDWT) <- c("d1", "d2", "d3", "d4", "d5", "d6", "d7", "s7")
  
  # now compare the energies for each scale (but not the smooth)
  
  energies <- vector()
  
  for(x in 1:7){
    energies[x] <- sum(priDWT[[x]]^2)/length(priDWT[[x]])
  }
  scaleNames <- c(1:7)

  # item category (durable good, nondurable good, or service)
  cn <- substr(priSeries$level[i], 1, 3)
  if(cn == "1.1"){ cat <- "Durable Good"}
  else if(cn == "1.2"){ cat <- "Nondurable Good"}
  else if(cn == "2.1"){ cat <- "Service"}

  itemTable <- rbind(itemTable, c(priSeries$name[i], cat, energies))
  class(itemTable[, 3])
  # plot the energies
  #barplot(energies ~ scaleNames, xlab = "Scale", main = priSeries$name[i])

  
  
}

colnames(itemTable) <- c("Item", "Category", "d1.energy", "d2.energy", "d3.energy", "d4.energy", "d5.energy", "d6.energy", "d7.energy")

itemTable[, 3:9] <- sapply(itemTable[, 3:9], as.numeric)

avgD1Energy <- mean(as.numeric(itemTable[, 2]))

avgAllEnergy <- mean(itemTable[, 3:9])

itemTable <- itemTable[order(as.numeric(itemTable$d1.energy)),]

#make the tables
require(pander)
panderOptions('table.split.table', Inf)
panderOptions('digits', 6)
panderOptions('round', 5)
pander(itemTable, style = 'rmarkdown')


```

The following simply gives the items' ranks in terms of energies at each scale (where rank 1 indicates lowest energy among all items at that scale).  The table is ordered by the fourth scale.

```{r rank items by scale energies}

# produce a data.frame of all the items from above with their 
# ranks in terms of energy at each scale

itemEnergyRanks <- itemTable

for(i in 3:9){
  itemEnergyRanks <- itemEnergyRanks[order(as.numeric(itemEnergyRanks[, i])),]
  itemEnergyRanks <- cbind(itemEnergyRanks, 1:nrow(itemEnergyRanks))
}

itemEnergyRanks <- itemEnergyRanks[, -(3:9)]

itemEnergyRanks <- itemEnergyRanks[order(itemEnergyRanks[, 6]),]

colnames(itemEnergyRanks)[3:9] <- c("d1en.rank", "d2en.rank", "d3en.rank", "d4en.rank", "d5en.rank", "d6en.rank", "d7en.rank")

#make the tables
require(pander)
panderOptions('table.split.table', Inf)
panderOptions('digits', 6)
panderOptions('round', 4)
pander(itemEnergyRanks, style = 'rmarkdown')

```



